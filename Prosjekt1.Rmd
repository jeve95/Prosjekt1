---
title: "Compulsory exercise 1: Group 12"
author: "Evgeni Vershinin, Kristina Ødegård"
date: "2024-02-08"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE,eval=TRUE)
library(MASS)
library(GGally)
library(ggfortify)
library(titanic)
library(pROC)
library(lattice)

set.seed(123)
```




# Problem 1  
## a) 
Quantative: Time, income earned, horsepower of a car.  
Qvalitative: Marital status, origin, Gender.  

## b)  
KNN, LDA, QDA can be used for multi-class classifications  

## c)  
### i-iii)  
The three components is called bias, variance and irreducible error.  
$(E[f (X) - \hat{f}(X)])^2$ represents the bias. It is the squared expectation between our model and the true data.  A high bias indicates that our model is
too simple and does not capture the complexities in our data(underfitting).  
$Var(\hat{f}(X))$ is the variance of our model. A high variance means our model is following our training data quite tight. It could mean our model too
complex, it might follow our training data quite good, but fail on test data(overfitting).  
$var(\epsilon)$ is the irreducible error. This error we have no control over. It is the inherit noise of the data, the randomness and natural variability of
the true data.  

### ii)  

Often by increasing the bias, we will reduce the variance, and vice verca. High variance could lead to higher score on training data, but if increased too
much it would also decrease the score on the test data(overfitting). A high bias could make our model more interpretative and give us somehow good score on
test data, but if too low it could make our model miss relevant relations between predictors and response(Underfitting). This is why we have a bias-variance
trade off.  

## d)  
The nereast neighbour for k=1 is a blue dot, so our classification is blue.  
For K=3, two of the nearest neighbors are red and 1 blue. This gives 2/3 red, so
it is red.  
For K=5 we have 3/5 red, so it is red.  


## e) 
### i)  


```{r setup2, echo=TRUE, eval=TRUE, cache=TRUE, fig.width=7, fig.height=3.5}


data(Boston)
data = Boston
model = lm(medv ~ rm + age,data=data)
summary(model)
```

### ii)  

```{r 1eii}
cor_matrix = cor(data.frame(data$medv, data$rm, data$age))
print(cor_matrix)
```

### iii)  

```{r 1eiii}
model2 = lm(medv ~ rm + age + nox, data=data)
summary(model2)
```


### iv)  
```{r 1eiv, fig.height=3, fig.width=6}
ggpairs(data.frame(data$medv, data$rm, data$age, data$nox))
```
  
  
Looking at the correlation between Age and NOX, it is 0.731, which is quite high which suggest it has  multicollinearity, which means they give some of the
same information for the model.  

# Problem 2  

## a)  

```{r Problem 2, echo=TRUE, eval=TRUE}

model3 = lm(medv ~ poly(rm, 2) + I(age*crim) + age + crim, data=data)
print(model3$coefficient)
valuechanged = (-10*(-0.796544)+60*(-0.067283)+0.005792 * (-10 + 60))*1000
```
If the crime is reduced by 10 and age is 60, and then considering all other factor are kept equal our median value of the property is changed by `r round(valuechanged,2)`.




## b)  

First thing we could do is to plot all our data with say ggpairs from GGAlly. We can then look how the data is spread and correlated. We want to carefully identify if we have any outliers. If most our points follow a specific pattern, but then one point stick out, we could carefully consider to remove it. One approach could be to look at the mean and std of a given data columns and if some point lie 2 standard deviations away from the mean we could remove them.

## c)  

```{r problem 2 c, echo=TRUE, eval=TRUE}
model4 <- lm(medv ~ crim + age + rm, data = Boston)
sm = summary(model4)
stderr = sm$coefficients[4, "Std. Error"]
```

### i)  

A t-value is calculated with the follow formula:
$t = \frac{Coefficient~Estimate}{Standard~Error} = \frac{10}{`r round(stderr,3)`}$
<!-- 10/0.40201=24.875 --> 

The t-value for rm if the estimated coefficient was 10, but with the same standard error is 24.89. <!-- Insert variable instead.--> 

A higher coefficient with standard error kept the same gives us a higher t value and higher t values makes our estimate for statistically significant, as the p value drops. The t distribution is like a normal distribution but with a estimated variance from the sample. With a low amount of sample the t distributions curve has fatter tails and lower around the mean, as sample increases it converges to a normal distribution. So a higher absolute t value puts our score further towards the tails, strengthening our result.

### ii)  

```{r problem 2 cii, echo=TRUE, eval=TRUE}
f_test <-anova(model4)
print(f_test)
```
An F-test was utilized to decide if at least one of the predictors is useful in predicting the response.

In this case, all three predictors has a p-value associated with the F-statisic that is $<2.2\cdot10^{-16}$. Since the p-value is essentially zero, we have strong evidence to reject the null hypothesis, which would be that none of the predictors were useful in predicting the response variable. Thus, at least one of the predictors is useful in predicting the response.

### iii)  

```{r problem 2 ciii, echo=TRUE, eval=TRUE}
model5 <- lm(medv ~ crim + age, data = Boston)
f_test_2 <-anova(model5)
print(f_test_2)
```
The p-value associated with F values, though increased for age in this model without the predictor rm, are still significantly lower than the  typical significance level of 0.05. Thus the null hypothesis can be rejected for both predictors. We conclude that the model is still useful.


## d)  

```{r problem 2 d, echo=TRUE, eval=TRUE}
new_data <- data.frame(crim = 10, age = 90, rm = 5)
# Predict the response variable using the model for the confidence interval
prediction_c <- predict(model4, newdata = new_data, interval = "confidence", level = 0.99)

# Extract lower and upper bounds from the confidence interval prediction
lower_bound_c <- prediction_c[1]
upper_bound_c <- prediction_c[2]

# Print the results for the confidence interval
cat("Lower bound of 99% confidence interval:", lower_bound_c, "\n")
cat("Upper bound of 99% confidence interval:", upper_bound_c, "\n\n")

# Predict the response variable using the model for the prediction interval
prediction_p <- predict(model4, newdata = new_data, interval = "prediction", level = 0.99)

# Extract lower and upper bounds from the prediction interval prediction
lower_bound_p <- prediction_p[1]
upper_bound_p <- prediction_p[2]

# Print the results for the prediction interval
cat("Lower bound of 99% prediction interval:", lower_bound_p, "\n")
cat("Upper bound of 99% prediction interval:", upper_bound_p, "\n")
```
### iii)  

The difference between confidence interval and prediction interval is that the confidence interval estimates the range in which the population mean response most likely will lie, and the prediction interval estimates the range in which a future individual observation most likely will fall within.

**iv) 
```{r problem 2 div, echo=TRUE, eval=TRUE, fig.width=6, fig.height=4, fig.align='center'}
# Create diagnostic plots
autoplot(model4, which = c(1, 2, 3, 5))
```

## e)  

### i)  

The gender is a binary qualitative variable. The student has used two binary variables to describe what only needs one. If two binary qualitative variables were used, it would in fact allow for four different cases:

\begin{align*}
& \text{Case 1: Male and Female} & & x_{male}=1 \text{ and } x_{female}=1 \\
& \text{Case 2: Male and Not Female} & & x_{male}=1 \text{ and } x_{female}=0 \\
& \text{Case 3: Not Male and Female} & & x_{male}=0 \text{ and } x_{female}=1 \\
& \text{Case 4: Not Male and Not Female} & & x_{male}=0 \text{ and } x_{female}=0 \\
\end{align*}

In reality, there are only two cases:
\begin{align*}
& \text{Case 1: Male (and Not Female)} & & x_{gender}=1 \\
& \text{Case 2: Female (and Not Male)} & & x_{gender}=0 \\
\end{align*}

### ii)  
The formula should only have one binary quantitative variable to describe 
the gender.

$y=\beta_{0}+\beta_{1}x_{gender}+\varepsilon$

where 

$x_{gender}= \begin{cases}1 & if~male\\0 & if~female\end{cases}$

**iii) A formula for a linear model that predicts income based on the predictor 
education degree with three categories {Bachelor, Master, PhD} is:

$y=\beta_{0}+\beta_{1}x_{education}+\varepsilon$

where

$x_{education}= \begin{cases}2 & if~PhD\\1 & if~Master\\0 & if~Bachelor\end{cases}$

## f)  

### i) True  

### ii) False  

### iii) True  

### iv) False  




# Problem 3  

## a)  

### i)  

```{r proble 3a, echo=TRUE, eval=TRUE}
data("titanic_train")
vars_to_be_removed <- c("PassengerId", "Name", "Ticket", "Cabin", "Embarked")
titanic_train <- titanic_train[, -which(names(titanic_train) %in% vars_to_be_removed)]
titanic_train$Pclass <- as.factor(titanic_train$Pclass)
train_idx <- sample(1:nrow(titanic_train), 0.8 * nrow(titanic_train))
titanic_test <- titanic_train[-train_idx, ]
titanic_train <- titanic_train[train_idx, ]
titanic_train <- na.omit(titanic_train)
titanic_test <- na.omit(titanic_test)
logReg = glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare, data = titanic_train, family = "binomial")
predicted_probabilities <- predict(logReg, newdata = titanic_test, type = "response")
predicted_labels <- ifelse(predicted_probabilities > 0.5, 1, 0)
actual_labels <- titanic_test$Survived 
misclassification_error <- mean(predicted_labels != actual_labels)
accuracy <- 1 - misclassification_error
print(round(accuracy,2))
```

### ii)  


```{r 3aii}
chitest = anova(logReg,test="Chisq")
chitest
```

Given that th p value is less that $2.2\cdot10^{-16}$ suggest passenger class is really relevant predictor. In fact it is our strongest predictor. $H_0$ is that our predictor could just as well be 0, a low p value would break this hypothesis. 

### iii)  


```{r 3aiii}
newdata = data.frame(Pclass=as.factor(c(1,3)),Sex=c("female","female"),Age=c(40,40),SibSp=c(1,1),Parch=c(0,0),Fare=c(200,20))
casepredict = predict(logReg,newdata = newdata,type="response")
casepredict
```

Comparing the a woman with the all the same factors, except for class and fare price, the higher class woman has a `r round(casepredict[1],2)` chance of survival, while the lower class woman has a `r round(casepredict[2],2)` chance of survival.

### iv)  


```{r problem 3a(lda),echo=TRUE,eval=TRUE}


ldamodel = lda(Survived ~ ., data = titanic_train)
print(ldamodel)
predicted <- predict(ldamodel, newdata = titanic_test)
predicted_labels <- predicted$class
actual_labels <- titanic_test$Survived 
misclassification_error = mean(predicted_labels != actual_labels)
accuracy <- 1 - misclassification_error
print(accuracy)
```

### v)  

```{r problem 1a(QDA), echo=TRUE,eval=TRUE}
qdamodel = qda(Survived ~ ., data = titanic_train)
print(qdamodel)
predicted <- predict(qdamodel, newdata = titanic_test)
predicted_labels <- predicted$class
actual_labels <- titanic_test$Survived 
misclassification_error = mean(predicted_labels != actual_labels)
accuracy <- 1 - misclassification_error
print(accuracy)


```

### vi)  

```{r problem 1a(ROC),fig.height=4,fig.width=4}


logreg_probs <- predict(logReg, newdata = titanic_test, type = "response")
lda_probs <- predict(ldamodel, newdata = titanic_test)$posterior[,2]  
qda_probs <- predict(qdamodel, newdata = titanic_test)$posterior[,2]
actual_outcomes <- titanic_test$Survived

# ROC for Logistic Regression
roc_logreg <- roc(actual_outcomes, logreg_probs)

# ROC for LDA
roc_lda <- roc(actual_outcomes, lda_probs)

# ROC for QDA
roc_qda <- roc(actual_outcomes, qda_probs)

# Plot ROC curve for Logistic Regression
plot(roc_logreg, main="ROC Curves Comparison", col="red", xlim=c(1, 0), ylim=c(0, 1))

# Add ROC curve for LDA
lines(roc_lda, col="blue")

# Add ROC curve for QDA
lines(roc_qda, col="green")

# Add legend
legend("bottomright", legend=c("Logistic Regression", "LDA", "QDA"),
       col=c("red", "blue", "green"), lwd=2)




# AUC for Logistic Regression
auc_logreg <- roc_logreg$auc

# AUC for LDA
auc_lda <- roc_lda$auc

# AUC for QDA
auc_qda <- roc_qda$auc
# Print AUC values
cat("AUC for Logistic Regression:", auc_logreg, "\n")
cat("AUC for LDA:", auc_lda, "\n")
cat("AUC for QDA:", auc_qda, "\n")

```


AUC over 0.8 is generally considered good. QDA performs slightly better than LDA and Logistic Regression. However Logistic regression is almost as accurate, but more interpretable, for one it gives clearer relationships between the predictors and they are linear.


## b)  

Diagnostic Paradigm:

The diagnostic paradigm in classification focuses on directly modeling the relationship between the predictors and the outcome (class labels. The primary goal is to diagnose the class of an observation based on its features. Models under this paradigm typically estimate the probability that an observation belongs to a certain class given its features. This approach is more deterministic, where the decision rule is often based on the estimated probabilities and predefined thresholds.
Sampling Paradigm:

The sampling paradigm, on the other hand, is based on modeling the distribution of features for each class. It involves estimating how the data is generated or sampled for each class and then uses this information to classify new observations. This approach is more generative, as it tries to understand the underlying data generation process for each class and uses this generative model to make classification decisions.

Key Differences:

Modeling Focus: The diagnostic paradigm focuses on the direct relationship between features and classes, often modeling the conditional probability of a class given the features. The sampling paradigm focuses on modeling the distribution of features for each class, effectively capturing how data for each class is generated.
Approach: Diagnostic models are more discriminative, directly seeking a decision boundary between classes. Sampling models are generative, aiming to understand the data generation process for each class.
Decision Rule: In the diagnostic paradigm, classification decisions are often made based on estimated probabilities and thresholds. In the sampling paradigm, decisions are made based on likelihoods derived from the modeled distributions of features for each class.

### ii)  
Classification Models and Their Paradigms

Logistic Regression: Belongs to the diagnostic paradigm. It models the probability of a class given the features directly using the logistic function.

KNN (K-Nearest Neighbors): Can be seen as part of the sampling paradigm. It classifies an observation based on the majority class among its K nearest neighbors, effectively using the local distribution of the data around an observation to make a classification decision.

Naive Bayes Classifier: Fits within the sampling paradigm. It is a generative model that assumes independence among features given the class and estimates the distribution of each feature within each class to compute the posterior probability of the class given an observation.

LDA (Linear Discriminant Analysis): Primarily fits within the sampling paradigm. LDA models the distribution of features in each class (assuming a Gaussian distribution with a common covariance matrix across classes) and uses this to classify new observations by finding the class that maximizes the posterior probability.

QDA (Quadratic Discriminant Analysis): Also belongs to the sampling paradigm. Similar to LDA, QDA models the distribution of features for each class, but it allows for class-specific covariance matrices, leading to quadratic decision boundaries. Like LDA, it uses the estimated distributions to compute class probabilities for classification.


## c)  


### i)  
The decision boundary is given by setting by taking the log of each pdf and adding the log of the prior probability equal each other.

\begin{align*}
X|\{Y=1\}\sim N(-2,1.5^2)\\
X|\{Y=2\}\sim N(2,1.5^2) \\
\pi_1 = 0.3 \\
\space \pi_2 =0.7
\end{align*}

\begin{align*}
g_1(x|\pi_1)=g_2(X|\pi_2) \\
  log(\frac{1}{\sqrt{2\pi}\sigma_1}+e^{-\frac{(x-\mu_1)^2}{2\sigma^2_1}})+log(\pi_1)=log(\frac{1}{\sqrt{2\pi}\sigma_2}+e^{-\frac{(x-\mu_2)^2}{2\sigma_2^2}})+log(\pi_2) \\
  log(\frac{1}{\sqrt{2\pi}1.5}+e^{-\frac{(x+2)^2}{2\cdot1.5^2}})+log(0.3)=log(\frac{1}{\sqrt{2\pi}1.5}+e^{-\frac{(x-2)^2}{2\cdot1.5^2}})+log(0.7) \\
  \frac{(x-2)^2-(x+2)^2}{4.5}+log(0.3)-log(0.7) = 0 \\
  \frac{x^2-4x+4-x^2-4x-4}{4.5}+log(\frac{0.3}{0.7}) = 0 \\
  \frac{-8x}{4.5}-0.847=0 \\
  x=-0.477
\end{align*}

For x=-0.47 is the point where the graphs cross, at this point a model would change it decision of which class our point belongs to.


```{r oppgave 3c, echo=TRUE,eval=TRUE}

# generate data for the two normal distributions
n_samples_class1 <- 3000
n_samples_class2 <- 7000
x1 <- rnorm(n_samples_class1, mean = -2, sd = 1.5)
x2 <- rnorm(n_samples_class2, mean = 2, sd = 1.5)
# create a data frame with the generated data
df <- data.frame(X1 = c(x1, x2), class = c(rep(1, n_samples_class1), rep(2, n_samples_class2)))


```




# Problem 4  

## a)  
iv)  True


## b)  

### i)  

Corrected three mistakes:

* Corrected the number of folds from 4 to 5.
*
* Corrected the RMSE formula.
  
```{r problem 4bi, echo=TRUE,eval=TRUE}
# Import the Boston housing price dataset
data(Boston)
# select specific variables
selected_vars <- c("crim", "rm", "age", "medv")
boston_selected <- Boston[, selected_vars]

# manually perform the 5-fold cross-validation
folds <- createFolds(boston_selected$medv, k = 5) # CORRECTED to 5
rmse_list <- list()
for (i in 1:length(folds)) {
# get the training and validation sets
train <- boston_selected[-folds[[i]], ] # CORRECTED
val <- boston_selected[-folds[[i]], ]
# fit a linear regression model
model <- lm(medv ~ ., data = train)
# compute RMSE on the validation set
pred <- predict(model, val)
rmse <- sqrt(mean((pred - val$medv)^2))  # CORRECTED squared error (RSME)
rmse <- rmse[1] # take out the value
# store rmse in rmse_list
rmse_list[[i]] <- rmse
}
# compute mean of rmse_list
rmse_mean <- mean(as.numeric(rmse_list))
cat("rmse_mean:", rmse_mean, "\n")
```

### ii) 
Changed:
folds <- createFolds(boston_selected$medv, k = 5) 

to:

folds <- createFolds(boston_selected$medv, k = nrow(boston_selected))

## c)
### i)
Corrected the following:

* Fixed the typo in the variable name "standard_erorr_of_the_median_bootstrap" to "standard_error_of_median_bootstrap".
* Changed the sample size (the second parameter of the sample function) from 1 to n. When bootstrapping, the sample size should be the same size as the original dataset, which facilitates the estimation of for example the median.
* Changed the replace variable of the sample function to TRUE to allow for an element to be selected multiple times in the same sample.
* Set seed before bootstrapping for reproducibility.

```{r problem 4ci, echo=TRUE,eval=TRUE}
# simulate data (no need to change this part)
set.seed(123) # CORRECTED
n <- 1000 # population size
dataset <- rnorm(n) # population
# bootstrap
B <- 10 # bootstrap sample size
boot <- matrix(NA, nrow = B, ncol = 1)
for (i in 1:B) {
  set.seed(123 + i) # CORRECTED
  boot[i, ] <- median(sample(dataset, n, replace = TRUE)) # CORRECTED
}
# compute the standard error of the median from the bootstrap samples
standard_error_of_the_median_bootstrap <- sd(boot) # CORRECTED
cat("standard_error_of_the_median_bootstrap:", standard_error_of_the_median_bootstrap, "\n")
```

### ii)
```{r problem 4cii, echo=TRUE,eval=TRUE}
# simulate data (no need to change this part)
set.seed(123)
n <- 1000 # population size
dataset <- rnorm(n) # population

# bootstrap with replacement
B <- 10 # bootstrap sample size
boot_with_replacement <- matrix(NA, nrow = B, ncol = 1)
for (i in 1:B) {
  set.seed(123 + i) # setting seed for reproducibility
  boot_with_replacement[i, ] <- median(sample(dataset, n, replace = TRUE))
}

# bootstrap without replacement
boot_without_replacement <- matrix(NA, nrow = B, ncol = 1)
for (i in 1:B) {
  set.seed(123 + i) # setting seed for reproducibility
  boot_without_replacement[i, ] <- median(sample(dataset, n, replace = FALSE))
}

# compute the standard errors of the medians
standard_error_with_replacement <- sd(boot_with_replacement)
standard_error_without_replacement <- sd(boot_without_replacement)

cat("Standard Error with replacement:", standard_error_with_replacement, "\n")
cat("Standard Error without replacement:", standard_error_without_replacement, "\n")
```
For the standard error without replacement, the same model is produced for each sample. That is because when sampling with the same size of the original data set without replacement, each sample naturally are the same. Thus, the standard error is 0 when bootstrapping without replacement. This indicates no variability in the standard error accros the different bootstrap samples, as all the samples are identical.

## d)

