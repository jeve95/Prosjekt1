---
title: "Compulsory exercise 1: Group 12"
author: "Evgeni Vershinin, Kristina Ødegård"
date: "2024-02-08"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(GGally)
library(ggfortify)
library(titanic)
library(pROC)
set.seed(123)
```




**Problem 1  
*a)  
Quantative: Time, income earned, horsepower of a car.
Qvalitative: Marital status, origin, Gender.  
*b) KNN, LDA, QDA can be used for multi-class classifications.  
*c)  
*d)The nereast neighbour for k=1 is a blue dot, so our classification is blue. For K=3, two of the nearest neighbors are red and 1 blue. This gives 2/3 red, so it is red. For K=5 we have 3/5 red, so it is red.


```{r setup2, echo=TRUE, eval=TRUE}
data(Boston)
data = Boston

model = lm(medv ~ rm + age,data=data)
summary(model)

cor_matrix = cor(data.frame(data$medv, data$rm, data$age))
print(cor_matrix)
model2 = lm(medv ~ rm + age + nox, data=data)
summary(model2)
ggpairs(data.frame(data$medv, data$rm, data$age, data$nox))
```


e)
IV
Looking at the correlation between Age and NOX, it is 0.731, which is quite high which suggest it has  multicollinearity, which means they give the some of the same information for the model.

***Problem 2
**a)

```{r Problem 2, echo=TRUE, eval=TRUE}

model3 = lm(medv ~ poly(rm, 2) + I(age*crim) + age + crim, data=data)
summary(model3)


valuechanged = (-10*(-0.796544)+60*(-0.067283)+0.005792 * (-10 + 60))*1000
```
If the crime is reduced by 10 and age is 60, and then considering all other factor keeps equal our median value of the property is changed by `r valuechanged`.`



```{r problem 2 b, echo=TRUE, eval=TRUE}
```

**b)
First thing we could do is to plot all our data with say ggpairs from GGAlly. We can then look how the data is spread and correlated. We want to carefully identify if we have any outliers. If most our points follow a specific pattern, but then one point stick out, we could carefully consider to remove it. One approach could be to look at the mean and std of a given data columns and if some point lie 2 standard deviations away from the mean we could remove them.

**c)
```{r problem 2 c, echo=TRUE, eval=TRUE}
model4 <- lm(medv ~ crim + age + rm, data = Boston)
summary(model4)
```

**i) A t-value is calculated with the follow formula:
$t = \frac{Coefficient~Estimate}{Standard~Error} = \frac{10}{`r round(model4$coefficients[3], 3)`}$
<!-- 10/0.40201=24.875 --> 

The t-value for rm if the estimated coefficient was 10, but with the same standard error is 24.89. <!-- Insert variable instead.--> 

The t-value is a measure of how many standard errors the estimated coefficient is from zero. This indicates how strong effect the variable has in the response variable. That is, how significant it is. 

When the coefficient estimate is increased, taking it further away from zero, it increases the t-value, making it more statistically significant.

**ii)
```{r problem 2 cii, echo=TRUE, eval=TRUE}
f_test <-anova(model4)
print(f_test)
```
An F-test was utilized to decide if at least one of the predictors is useful in predicting the response.

In this case, all three predictors has a p-value associated with the F-statisic that is $<2.2 ^{-16}$. Since the p-value is essentially zero, we have strong evidence to reject the null hypothesis, which would be that none of the predictors were useful in predicting the response variable. Thus, at least one of the predictors is useful in predicting the response.

**iii)
```{r problem 2 ciii, echo=TRUE, eval=TRUE}
model5 <- lm(medv ~ crim + age, data = Boston)
f_test_2 <-anova(model5)
print(f_test_2)
```
The p-value associated with F values, though increased for age in this model without the predictor rm, are still significantly lower than the  typical significance level of 0.05. Thus the null hypothesis can be rejected for both predictors. We conclude that the model is still useful.

```{r problem 2 d, echo=TRUE, eval=TRUE}
new_data <- data.frame(crim = 10, age = 90, rm = 5)
# Predict the response variable using the model for the confidence interval
prediction_c <- predict(model4, newdata = new_data, interval = "confidence", level = 0.99)

# Extract lower and upper bounds from the confidence interval prediction
lower_bound_c <- prediction_c[1]
upper_bound_c <- prediction_c[2]

# Print the results for the confidence interval
cat("Lower bound of 99% confidence interval:", lower_bound_c, "\n")
cat("Upper bound of 99% confidence interval:", upper_bound_c, "\n\n")

# Predict the response variable using the model for the prediction interval
prediction_p <- predict(model4, newdata = new_data, interval = "prediction", level = 0.99)

# Extract lower and upper bounds from the prediction interval prediction
lower_bound_p <- prediction_p[1]
upper_bound_p <- prediction_p[2]

# Print the results for the prediction interval
cat("Lower bound of 99% prediction interval:", lower_bound_p, "\n")
cat("Upper bound of 99% prediction interval:", upper_bound_p, "\n")
```
**iii)
The difference between confidence interval and prediction interval is that the confidence interval estimates the range in which the population mean response most likely will lie, and the prediction interval estimates the range in which a future individual observation most likely will fall within.

**iv) 
```{r problem 2 div, echo=TRUE, eval=TRUE}

# Create diagnostic plots
autoplot(model4, which = c(1, 2, 3, 5))
```
**e)
**i) The gender is a binary qualitative variable. The student has used two binary variables to describe what only needs one.
$y= \beta_{0} + \beta_{1}x_{gender} +  \varepsilon$
$x_{gender}= \begin{cases}1 & if~male\\0 & if~female\end{cases}$







***Problem 3
a)

```{r proble 3a, echo=TRUE, eval=TRUE}
data("titanic_train")
vars_to_be_removed <- c("PassengerId", "Name", "Ticket", "Cabin", "Embarked")
View(titanic_train)
titanic_train <- titanic_train[, -which(names(titanic_train) %in% vars_to_be_removed)]
titanic_train$Pclass <- as.factor(titanic_train$Pclass)
train_idx <- sample(1:nrow(titanic_train), 0.8 * nrow(titanic_train))
titanic_test <- titanic_train[-train_idx, ]
titanic_train <- titanic_train[train_idx, ]
titanic_train <- na.omit(titanic_train)
titanic_test <- na.omit(titanic_test)
logReg = glm(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare, data = titanic_train, family = "binomial")
summary(logReg)
predicted_probabilities <- predict(logReg, newdata = titanic_test, type = "response")
predicted_labels <- ifelse(predicted_probabilities > 0.5, 1, 0)
actual_labels <- titanic_test$Survived 
misclassification_error <- mean(predicted_labels != actual_labels)
accuracy <- 1 - misclassification_error
print(accuracy)
chitest = anova(logReg,test="Chisq")
chitest
newdata = data.frame(Pclass=as.factor(c(1,3)),Sex=c("female","female"),Age=c(40,40),SibSp=c(1,1),Parch=c(0,0),Fare=c(200,20))
newdata
casepredict = predict(logReg,newdata = newdata,type="response")
casepredict
```

Given a p value of less that 2.2*10^(-16) suggest it is really relevant predictor. Given that $H_0$ is predictor could just as well be 0, a low p value would break this hypothesis. Comparing the a woman with the all the same factors, except for class and fare price, the higher class woman has a `r casepredict[1]` chance of survival, while the lower class woman has a `r casepredict[2]` chance of survival.


```{r problem 3a(lda),echo=TRUE,eval=TRUE}


ldamodel = lda(Survived ~ ., data = titanic_train)
print(ldamodel)
predicted <- predict(ldamodel, newdata = titanic_test)
predicted_labels <- predicted$class
actual_labels <- titanic_test$Survived 
misclassification_error = mean(predicted_labels != actual_labels)
accuracy <- 1 - misclassification_error
print(accuracy)
```






```{r problem 1a(QDA), echo=TRUE,eval=TRUE}
qdamodel = qda(Survived ~ ., data = titanic_train)
print(qdamodel)
predicted <- predict(qdamodel, newdata = titanic_test)
predicted_labels <- predicted$class
actual_labels <- titanic_test$Survived 
misclassification_error = mean(predicted_labels != actual_labels)
accuracy <- 1 - misclassification_error
print(accuracy)


```



```{r problem 1a(ROC), echo=TRUE,eval=TRUE}


logreg_probs <- predict(logReg, newdata = titanic_test, type = "response")
lda_probs <- predict(ldamodel, newdata = titanic_test)$posterior[,2]  
qda_probs <- predict(qdamodel, newdata = titanic_test)$posterior[,2]
actual_outcomes <- titanic_test$Survived

# ROC for Logistic Regression
roc_logreg <- roc(actual_outcomes, logreg_probs)

# ROC for LDA
roc_lda <- roc(actual_outcomes, lda_probs)

# ROC for QDA
roc_qda <- roc(actual_outcomes, qda_probs)

# Plot ROC curve for Logistic Regression
plot(roc_logreg, main="ROC Curves Comparison", col="red")

# Add ROC curve for LDA
lines(roc_lda, col="blue")

# Add ROC curve for QDA
lines(roc_qda, col="green")

# Add legend
legend("bottomright", legend=c("Logistic Regression", "LDA", "QDA"),
       col=c("red", "blue", "green"), lwd=2)




# AUC for Logistic Regression
auc_logreg <- roc_logreg$auc

# AUC for LDA
auc_lda <- roc_lda$auc

# AUC for QDA
auc_qda <- roc_qda$auc
# Print AUC values
cat("AUC for Logistic Regression:", auc_logreg, "\n")
cat("AUC for LDA:", auc_lda, "\n")
cat("AUC for QDA:", auc_qda, "\n")

```


AUC over 0.8 is generally considered good. QDA performs slightly better than LDA and Logistic Regression. However Logistic regression is almost as accurate, but more interpretable, for one it gives clearer relationships between the predictors and they are linear.


**b)

Diagnostic Paradigm:

The diagnostic paradigm in classification focuses on directly modeling the relationship between the predictors and the outcome (class labels. The primary goal is to diagnose the class of an observation based on its features. Models under this paradigm typically estimate the probability that an observation belongs to a certain class given its features. This approach is more deterministic, where the decision rule is often based on the estimated probabilities and predefined thresholds.
Sampling Paradigm:

The sampling paradigm, on the other hand, is based on modeling the distribution of features for each class. It involves estimating how the data is generated or sampled for each class and then uses this information to classify new observations. This approach is more generative, as it tries to understand the underlying data generation process for each class and uses this generative model to make classification decisions.

Key Differences:

Modeling Focus: The diagnostic paradigm focuses on the direct relationship between features and classes, often modeling the conditional probability of a class given the features. The sampling paradigm focuses on modeling the distribution of features for each class, effectively capturing how data for each class is generated.
Approach: Diagnostic models are more discriminative, directly seeking a decision boundary between classes. Sampling models are generative, aiming to understand the data generation process for each class.
Decision Rule: In the diagnostic paradigm, classification decisions are often made based on estimated probabilities and thresholds. In the sampling paradigm, decisions are made based on likelihoods derived from the modeled distributions of features for each class.

ii) Classification Models and Their Paradigms

Logistic Regression: Belongs to the diagnostic paradigm. It models the probability of a class given the features directly using the logistic function.

KNN (K-Nearest Neighbors): Can be seen as part of the sampling paradigm. It classifies an observation based on the majority class among its K nearest neighbors, effectively using the local distribution of the data around an observation to make a classification decision.

Naive Bayes Classifier: Fits within the sampling paradigm. It is a generative model that assumes independence among features given the class and estimates the distribution of each feature within each class to compute the posterior probability of the class given an observation.

LDA (Linear Discriminant Analysis): Primarily fits within the sampling paradigm. LDA models the distribution of features in each class (assuming a Gaussian distribution with a common covariance matrix across classes) and uses this to classify new observations by finding the class that maximizes the posterior probability.

QDA (Quadratic Discriminant Analysis): Also belongs to the sampling paradigm. Similar to LDA, QDA models the distribution of features for each class, but it allows for class-specific covariance matrices, leading to quadratic decision boundaries. Like LDA, it uses the estimated distributions to compute class probabilities for classification.























