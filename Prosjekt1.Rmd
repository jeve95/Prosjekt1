---
title: "Compulsory exercise 1: Group 12"
author: "Evgeni Vershinin, Kristina Ødegård"
date: "2024-02-08"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Problem 1  
*a)  
Quantative: Time, income earned, horsepower of a car.
Qvalitative: Marital status, origin, Gender.  
*b) KNN, LDA, QDA can be used for multi-class classifications.  
*c)  
*d)The nereast neighbour for k=1 is a blue dot, so our classification is blue. For K=3, two of the nearest neighbors are red and 1 blue. This gives 2/3 red, so it is red. For K=5 we have 3/5 red, so it is red.


```{r setup2, echo=TRUE, eval=TRUE}
library(MASS)
data(Boston)
data = Boston

model = lm(medv ~ rm + age,data=data)
summary(model)

cor_matrix = cor(data.frame(data$medv, data$rm, data$age))
print(cor_matrix)
model2 = lm(medv ~ rm + age + nox, data=data)
summary(model2)
#library(GGally)
#ggpairs(data.frame(data$medv, data$rm, data$age, data$nox))
```


e)
IV
Looking at the correlation between Age and NOX, it is 0.731, which is quite high which suggest it has  multicollinearity, which means they give the some of the same information for the model.

***Problem 2
**a)

```{r Problem 2, echo=TRUE, eval=TRUE}

model3 = lm(medv ~ poly(rm, 2) + I(age*crim) + age + crim, data=data)
summary(model3)
?Boston


valuechanged = (-10*(-0.796544)+60*(-0.067283)+0.005792 * (-10 + 60))*1000
```
If the crime is reduced by 10 and age is 60, and then considering all other factor keeps equal our median value of the property is changed by `r valuechanged`.`



```{r problem 2 b, echo=TRUE, eval=TRUE}
```

**b)
First thing we could do is to plot all our data with say ggpairs from GGAlly. We can then look how the data is spread and correlated. We want to carefully identify if we have any outliers. If most our points follow a specific pattern, but then one point stick out, we could carefully consider to remove it. One approach could be to look at the mean and std of a given data columns and if some point lie 2 standard deviations away from the mean we could remove them.

**c)
```{r problem 2 c, echo=TRUE, eval=TRUE}
model4 <- lm(medv ~ crim + age + rm, data = Boston)
summary(model4)
```

**i) A t-value is calculated with the follow formula:
$t = \frac{Coefficient~Estimate}{Standard~Error} = \frac{10}{`r round(model4$coefficients[3], 3)`}$
<!-- 10/0.40201=24.875 --> 

The t-value for rm if the estimated coefficient was 10, but with the same standard error is 24.89. <!-- Insert variable instead.--> 

The t-value is a measure of how many standard errors the estimated coefficient is from zero. This indicates how strong effect the variable has in the response variable. That is, how significant it is. 

When the coefficient estimate is increased, taking it further away from zero, it increases the t-value, making it more statistically significant.

**ii)
```{r problem 2 cii, echo=TRUE, eval=TRUE}
f_test <-anova(model4)
print(f_test)
```
An F-test was utilized to decide if at least one of the predictors is useful in predicting the response.

In this case, all three predictors has a p-value associated with the F-statisic that is $<2.2 ^{-16}$. Since the p-value is essentially zero, we have strong evidence to reject the null hypothesis, which would be that none of the predictors were useful in predicting the response variable. Thus, at least one of the predictors is useful in predicting the response.

**iii)
```{r problem 2 ciii, echo=TRUE, eval=TRUE}
model5 <- lm(medv ~ crim + age, data = Boston)
f_test_2 <-anova(model5)
print(f_test_2)
```
The p-value associated with F values, though increased for age in this model without the predictor rm, are still significantly lower than the  typical significance level of 0.05. Thus the null hypothesis can be rejected for both predictors. We conclude that the model is still useful.

```{r problem 2 d, echo=TRUE, eval=TRUE}
new_data <- data.frame(crim = 10, age = 90, rm = 5)
# Predict the response variable using the model for the confidence interval
prediction_c <- predict(model4, newdata = new_data, interval = "confidence", level = 0.99)

# Extract lower and upper bounds from the confidence interval prediction
lower_bound_c <- prediction_c[1]
upper_bound_c <- prediction_c[2]

# Print the results for the confidence interval
cat("Lower bound of 99% confidence interval:", lower_bound_c, "\n")
cat("Upper bound of 99% confidence interval:", upper_bound_c, "\n\n")

# Predict the response variable using the model for the prediction interval
prediction_p <- predict(model4, newdata = new_data, interval = "prediction", level = 0.99)

# Extract lower and upper bounds from the prediction interval prediction
lower_bound_p <- prediction_p[1]
upper_bound_p <- prediction_p[2]

# Print the results for the prediction interval
cat("Lower bound of 99% prediction interval:", lower_bound_p, "\n")
cat("Upper bound of 99% prediction interval:", upper_bound_p, "\n")
```
**iii)
The difference between confidence interval and prediction interval is that the confidence interval estimates the range in which the population mean response most likely will lie, and the prediction interval estimates the range in which a future individual observation most likely will fall within.

**iv) 
```{r problem 2 div, echo=TRUE, eval=TRUE}
library(ggfortify)
# Create diagnostic plots
autoplot(model4, which = c(1, 2, 3, 5))
```
**e)
**i) The gender is a binary qualitative variable. The student has used two binary variables to describe what only needs one.
$y= \beta_{0} + \beta_{1}x_{gender} +  \varepsilon$
$x_{gender}= \begin{cases}1 & if~male\\0 & if~female\end{cases}$

